---
author: appleboy
type: post
date: 2019-08-14T12:13:42+00:00
draft: true
private: true
url: /2019/08/7434/
dsq_thread_id:
  - 7586751587
categories:
  - blog

---
# Back-propagation

## 基本介紹

在 Neural Network 裡面使用 Gradient Descent 更新上百萬個參數，對於上百萬維度的 Vector 如何有效地計算偏微分出來，這就是 Backpropagation 做的事情，而 Backward Propagation 主要分為兩個步驟 Forward Phase 與 Backward Phase。也就是要先從 input 把值傳到 output，再從 output 往回傳遞 error 到每一層的神經元，去更新層與層之間權重的參數。

用一個簡單的例子來認識 `Backward Propagation`，我個人先用邏輯閘 (logic gates) 來當作範例，並且用 neural networks 來幫助學習，底下可以看到簡單的邏輯閘輸入及輸出

| X1 | X2 | Y |
| -- | -- | - |
|    |    |   |
|    | 1  | 1 |
| 1  |    | 1 |
| 1  | 1  |   |

根據上表的結果，可以知道只要 X, Y 相同值則輸出 0，若為不同值則輸出 1，所以可以畫成底下分佈在圖中四點

<!--more-->

![xor-output][1] 

## 系統資訊

  * 作業系統: MacOS 10.13.6
  * Python 版本: 3.7.3

## 實作流程

首先要準備輸入及輸出資料，程式碼如下:

<pre><code class="language-python">def generate_XOR_easy():
  inputs = []
  labels = []
  for i in range(11):
    inputs.append([0.1*i, 0.1*i])
    labels.append(0)

    if 0.1*i == 0.5:
      continue

    inputs.append([0.1*i, 1-0.1*i])
    labels.append(1)
  return np.array(inputs), np.array(labels).reshape(21, 1)</code></pre>

由上面程式碼可以看出來，除了 0.5 之外，只要 `X,Y` 不同則輸出 1，反之則輸出 0。可以用兩個變數將結果存起來

<pre><code class="language-python">inputs, expected_output = generate_XOR_easy()</code></pre>

建立 neural network model，用兩個輸入一個輸出，中間兩層 hidden layer

![hidden-layer][2] 

建立此 network 之前，需要預設 weights (權重) 及 biases (偏差值)，所以總共需要三組 (`hidden layer * 2 + output * 1`)不同的預設值

<pre><code class="language-python">inputLayerNeurons, hiddenLayerNeurons01, hiddenLayerNeurons02, outputLayerNeurons = 2,4,4,1

# Random weights and bias initialization
hidden_weights01 = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons01))
hidden_bias01 =np.random.uniform(size=(1,hiddenLayerNeurons01))
hidden_weights02 = np.random.uniform(size=(hiddenLayerNeurons01,hiddenLayerNeurons02))
hidden_bias02 =np.random.uniform(size=(1,hiddenLayerNeurons02))
output_weights = np.random.uniform(size=(hiddenLayerNeurons01,outputLayerNeurons))
output_bias = np.random.uniform(size=(1,outputLayerNeurons))</code></pre>

上面都是預設值，透過每一次每一次的 Forward Phase 及 Backward Propagation 計算後，修正初始值 (weights 跟 biases)。

### Forward Phase

在 Forward Phase 時，先從 input 將值一層層傳遞到 output。將一筆訓練資料 x1, x2 和 bias b 輸入到神經元 n 到輸出的過程，分成兩步，分別為 <code class="language-katex katex-inline">n_{in}</code>, $ n_{out} $，過程如下：

<pre><code class="language-katex">n_{in} = w_{1} x_{1}+w_{2}  x_{2}+w_{b}</code></pre>

<pre><code class="language-katex">n_{out} = \frac{1} {1+e^{-n_{in} } }</code></pre>

在輸入神經元時，$ n_{in} $ 先將 input 值和其權重作乘積。在輸出神經元時，$ n_{out} $ 將 $ n_{in} $ 的值用 `sigmoid` function 轉成值範圍從 0 到 1 的函數。

$$ n\_{out} = \frac{1}{1+e^{-(w\_0x\_0 + w\_1x\_1 + w\_2)}} $$

可以看到 `Sigmoidal Curve` 如下:

![sigmoid][3] 

傳遞到 $ n_{out} $ 後，可與訓練資料的答案 $ y $ 用 `cost function` 來計算其差值，並用 backward propagation 修正權重 $w\_1$、$w\_2$ 和 $w_b$ 。底下透過 python 定義 forwar function:

<pre><code class="language-python">def forward(inputs, weights, bias):
  layer_activation = np.dot(inputs, weights)
  layer_activation += bias
  layer_output = sigmoid(layer_activation)
  return layer_output</code></pre>

假設一個簡單的類神經網路，共有兩層，四個神經元，其值傳遞的過程如下：

<pre><code class="language-math">n_{11(in)} = w_{11,x} x+w_{11,y} y+w_{11,b} \\
n_{12(in)} = w_{12,x} x+w_{12,y} y+w_{12,b} \\
n_{11(out)} = \frac{1} {1+e^{-n_{11(in)} } } \\
n_{12(out)} = \frac{1} {1+e^{-n_{12(in)} } } \\</code></pre>

其中 $n_{11(in)}$ 表示傳入神經元 $n_{11}$ 的值，而 $n_{11(out)}$ 表示傳出神經元 $n_{11}$ 的值，而 $w_{11,x}$ 表示值從 $x$ 傳入 $n_{11}$ 時，所乘上的權重。

第一層神經元將其輸出值 $n_{11(out)}$ 和 $n_{12(out)}$ 傳到第二層神經元 $n_{21}$ 和 $n_{22}$：

<pre><code class="language-math">n_{21(in)} = w_{21,11} n_{11(out)} + w_{21,12} n_{12(out)}+w_{21,b} \\
n_{22(in)} = w_{22,11} n_{11(out)} + w_{22,12} n_{12(out)}+w_{22,b} \\
n_{21(out)} = \frac{1} {1+e^{-n_{21(in)} } } \\
n_{22(out)} = \frac{1} {1+e^{-n_{22(in)} } } \\</code></pre>

傳遞完後，可與訓練資料的答案 $z\_1$ 和 $z\_2$ 用 `cost function` 來計算其差值，並用 `backward propagation` 修正權重。來看看如何用 python 算出 hidden layer，透過 forward 函式算出每一層的 layout 輸出。

<pre><code class="language-python"># hidden layer 01
hidden_layer01_output = forward(inputs, hidden_weights01, hidden_bias01)
# hidden layer 02
hidden_layer02_output = forward(hidden_layer01_output, hidden_weights02, hidden_bias02)
# outout layer
predicted_output = forward(hidden_layer02_output, output_weights, output_bias</code></pre>

### 計算 error

一個簡單的神經元 $n$，將一筆訓練資料 $x_1$,$x_2$ 傳遞到 $n_{out}$ 所得出的值和 $y$ 的值做比較，我們可用以下的 `cost function` 來計算：

$$ Loss = \sum(n_{out} &#8211; y)^2 $$

在這個函式中 $n_{out}$ 代表預測結果，而 $y$ 代表預期結果，根據兩個相減運算後的結果要越低越好，所以每一次的訓練都要修正 $w$ 及 $b$ 兩個值

![loss weight][4] 圖片來自: [Kabir Shah][5]

如何透過 Python 實作 `loss function`，程式碼如下:

<pre><code class="language-python"># One way of representing the loss function is by using the mean sum squared loss function:
loss = np.mean(np.square(expected_output - predicted_output))</code></pre>

## Backward Phase

上述方法稱為梯度下降。通過了解改變原有權重的方式，我們的輸出會變得更準確。底下會透過 Backward 方式來計算並修正原有的 `weights` 跟 `biases`。首先定義 `sigmoid derivative` 函式

<pre><code class="language-python">def sigmoid_derivative(x):
  return np.multiply(x, 1.0 - x)</code></pre>

接著從預測輸出及實際輸出結果找到誤差範圍，運用上述 `sigmoid derivative` 計算最終預測結果

<pre><code class="language-python">error = expected_output - predicted_output
d_predicted_output = error * sigmoid_derivative(predicted_output)</code></pre>

算完 output 預測結果後，接著透過每一層 weights 倒推算 Hidden layer 的輸出變化，底下定義一個 Backward function

<pre><code class="language-python">def backward(input_layer, output_layer, output_weights):
  error_layer = output_layer.dot(output_weights.T)
  layer_out = error_layer * sigmoid_derivative(input_layer)
  return layer_out</code></pre>

從上述函示可以推倒每一層 hidden layer 產生新的 Output 結果

<pre><code class="language-python">d_hidden_layer02 = backward(hidden_layer02_output, d_predicted_output, output_weights)
d_hidden_layer01 = backward(hidden_layer01_output, d_hidden_layer02, hidden_weights02)</code></pre>

計算最後一層 hidden layer 的 output，需要拿最後輸出結果及 weights 搭配原本的 output 來計算新的結果。拿到新的 hidden layer output 後，就可以計算每一層新的 weights 跟 biaes

<pre><code class="language-python"># adjusting second set (hidden --&gt; output) weights
output_weights += hidden_layer02_output.T.dot(d_predicted_output) * lr
output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr</code></pre>

其中 `lr` 為 learning rate，用來控制訓練的速度。有了新的 weights 跟 biaes 後，一樣回到 forward phase 階段一直重複計算，就可以看到 `loss` 值會持續下降，每 5000 epoch 就把 loss 值印出來。

<pre><code class="language-python"># print loss value
if i%5000 == 0 and i &gt; 5000:
  print("epoch %d loss: %f" % (i, loss))</code></pre>

## 輸出結果

執行底下指令跑 Linear 輸入輸出結果

<pre><code class="language-bash">python3.7 main.py</code></pre>

![linear-loss][6] 

<pre><code class="language-bash">Initial hidden weights 01: [0.84565293 0.6834652  0.85094491 0.41920999] [0.17736785 0.52481814 0.45242458 0.04695909]
Initial hidden biases 01: [0.21430572 0.29735269 0.55582146 0.83802454]
Initial hidden weights 02: [0.58295068 0.61186264 0.34277141 0.59017988] [0.19972425 0.9587604  0.43350049 0.6359824 ] [0.17636618 0.17935514 0.4067334  0.39973801] [0.39759216 0.22510757 0.66742286 0.28607256]
Initial hidden biases 02: [0.35450829 0.01242758 0.8618276  0.99002595]
Initial output weights: [0.4539586] [0.50013898] [0.3328481] [0.22857201]
Initial output biases: [0.06694125]
epoch 10000 loss: 0.005821
epoch 15000 loss: 0.005656
epoch 20000 loss: 0.000291
epoch 25000 loss: 0.000107
epoch 30000 loss: 0.000062
epoch 35000 loss: 0.000042
epoch 40000 loss: 0.000032
epoch 45000 loss: 0.000025
epoch 50000 loss: 0.000021
epoch 55000 loss: 0.000018
epoch 60000 loss: 0.000015
epoch 65000 loss: 0.000013
epoch 70000 loss: 0.000012
epoch 75000 loss: 0.000011
epoch 80000 loss: 0.000010
epoch 85000 loss: 0.000009
epoch 90000 loss: 0.000008
epoch 95000 loss: 0.000008
Final hidden weights 01: [  0.75834512 -26.36153941  11.12364589   9.29601692] [ -1.08284515  26.59260763 -11.16359884  -9.2386293 ]
Final hidden biases 01: [-1.67205889 -0.1031003   0.02952189 -0.0158684 ]
Final hidden weights 02: [ 0.48211773 -0.77884212 -0.01780437  0.01881963] [-5.38978048 15.80023661 -3.83441293 -1.53832531] [ 2.42390238 -6.88219818  1.7366772   0.42786864] [ 2.2914376  -5.69158731  1.74568547  0.22047292]
Final hidden biases 02: [ 0.28319369 -1.43384703  0.19998811 -0.26822989]
Final output weights: [-5.61521427] [15.62907175] [-4.07529034] [-1.503433]
Final output biases: [-2.31957977]</code></pre>

![linear][7] 

執行底下指令跑 XOR 輸入輸出結果

<pre><code class="language-bash">python3.7 main.py XOR</code></pre>

![xor-loss][8] 

<pre><code class="language-bash">Initial hidden weights 01: [0.92281341 0.60428908 0.63186045 0.89539761] [0.09672588 0.19145389 0.583335   0.28551392]
Initial hidden biases 01: [0.8675718  0.66717119 0.6725929  0.94920321]
Initial hidden weights 02: [0.3247088  0.36868658 0.93723077 0.30527878] [0.59957544 0.05342852 0.5094746  0.12137623] [0.7179039  0.9281696  0.94587981 0.74750085] [0.83338283 0.14326754 0.56453329 0.87715198]
Initial hidden biases 02: [0.68246195 0.0160206  0.06741691 0.51325857]
Initial output weights: [0.87895968] [0.45809581] [0.16232022] [0.88010558]
Initial output biases: [0.63543909]
epoch 10000 loss: 0.000131
epoch 15000 loss: 0.000064
epoch 20000 loss: 0.000039
epoch 25000 loss: 0.000028
epoch 30000 loss: 0.000021
epoch 35000 loss: 0.000016
epoch 40000 loss: 0.000014
epoch 45000 loss: 0.000011
epoch 50000 loss: 0.000010
epoch 55000 loss: 0.000009
epoch 60000 loss: 0.000008
epoch 65000 loss: 0.000007
epoch 70000 loss: 0.000006
epoch 75000 loss: 0.000006
epoch 80000 loss: 0.000005
epoch 85000 loss: 0.000005
epoch 90000 loss: 0.000004
epoch 95000 loss: 0.000004
Final hidden weights 01: [11.76967618  0.66139359  7.52222655  1.62816071] [-10.83109971   0.91658159  -6.53288044   3.4605012 ]
Final hidden biases 01: [-0.65848537  0.60137433 -0.74250014 -1.44369897]
Final hidden weights 02: [-2.87072665  6.35879723 12.80274899 -2.26610804] [ 0.54399695 -0.95651264 -1.46342264  0.12270451] [-1.32983183  4.31410336  7.55343856 -0.89982538] [ 1.33132734 -2.13434453 -4.01157671  1.3796778 ]
Final hidden biases 02: [ 0.95770724 -2.80654524 -5.46164581  0.85814771]
Final output weights: [4.78294802] [-7.4155687] [-15.69086295] [3.96517637]
Final output biases: [5.48341673]</code></pre>

![xor][9] 

## 心得

寫完這次作業對於 Machine Learning 有些基礎的了解，但是雖然了解 Back-propagation 的機制及寫法，我個人覺得不知道該如何把程式碼套用在公司的專案，或者是找到一些實際的案例透過 Back-propagation 解決問題。

 [1]: ./images/xor-output.png
 [2]: ./images/hidden-layer.png
 [3]: ./images/sigmoidal.png
 [4]: ./images/loss-weight.png
 [5]: https://kabir.ml/
 [6]: ./images/linear-loss-1.png
 [7]: ./images/linear.png
 [8]: ./images/xor-loss-1.png
 [9]: ./images/xor.png